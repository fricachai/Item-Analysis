# app.py
# -*- coding: utf-8 -*-
import io
import os
import re
import math
import traceback

import numpy as np
import pandas as pd
import streamlit as st
import statsmodels.api as sm
from scipy.stats import norm
from scipy.stats import pearsonr
from analysis import run_item_analysis, normalize_item_columns


# ---- Optional GPT report (if gpt_report.py exists & has generate_gpt_report) ----
GPT_AVAILABLE = False
generate_gpt_report = None
try:
    from gpt_report import generate_gpt_report  # type: ignore
    GPT_AVAILABLE = callable(generate_gpt_report)
except Exception:
    GPT_AVAILABLE = False
    generate_gpt_report = None


# ---- Page ----
st.set_page_config(page_title="Scale Item Analysis MVP", layout="wide")
st.title("ğŸ“Š Scale Item Analysis MVP")


# ---- Helpers ----
def read_csv_safely(uploaded_file) -> pd.DataFrame:
    """
    Robust CSV loader for Streamlit UploadedFile.
    Tries common encodings and handles BOM.
    """
    if uploaded_file is None:
        raise ValueError("å°šæœªä¸Šå‚³ CSV æª”æ¡ˆã€‚")

    raw = uploaded_file.getvalue()
    if raw is None or len(raw) == 0:
        raise ValueError("ä¸Šå‚³çš„æª”æ¡ˆæ˜¯ç©ºçš„ï¼ˆ0 bytesï¼‰ã€‚è«‹ç¢ºèª CSV å…§å®¹æ˜¯å¦å­˜åœ¨ã€‚")

    encodings = ["utf-8-sig", "utf-8", "cp950", "big5", "latin-1"]
    last_err = None
    for enc in encodings:
        try:
            bio = io.BytesIO(raw)
            return pd.read_csv(bio, encoding=enc)
        except Exception as e:
            last_err = e

    raise ValueError(f"è®€å– CSV å¤±æ•—ï¼ˆå·²å˜—è©¦ {encodings}ï¼‰ã€‚æœ€å¾ŒéŒ¯èª¤ï¼š{repr(last_err)}")


def safe_show_exception(e: Exception):
    st.error("ç™¼ç”ŸéŒ¯èª¤ï¼ˆsafeï¼‰")
    st.code(repr(e))
    with st.expander("Tracebackï¼ˆé™¤éŒ¯ç”¨ï¼‰"):
        st.code(traceback.format_exc())


def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    """
    Excel-friendly: UTF-8 with BOM
    """
    return df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig")


# ===== Item code detection =====
ITEM_CODE_RE = re.compile(r"^[A-Za-z]\d{2,3}(_\d+)?$")


def _find_item_cols(df: pd.DataFrame) -> list[str]:
    cols: list[str] = []
    for c in df.columns:
        s = str(c).strip()
        if ITEM_CODE_RE.match(s):
            cols.append(s)
    return cols


def _dim_letter(code: str) -> str | None:
    m = re.match(r"^([A-Za-z])", str(code))
    return m.group(1).upper() if m else None


def build_dim_means_per_row(df_norm: pd.DataFrame) -> pd.DataFrame:
    """
    ç”¢ç”Ÿé€åˆ—ï¼ˆæ¯ä»½å•å·ä¸€åˆ—ï¼‰çš„æ§‹é¢å¹³å‡ï¼š
    - ä¾é¡Œé …ä»£ç¢¼ç¬¬ä¸€ç¢¼æ±ºå®šæ§‹é¢ï¼ˆA/B/C...ï¼‰
    - æ¯åˆ—å°è©²æ§‹é¢æ‰€æœ‰é¡Œç›®åš mean(axis=1, skipna=True)
    - è¼¸å‡ºç‚ºã€Œ4 ä½å°æ•¸å­—ä¸²ã€ï¼Œæœªæ»¿è£œ 0ï¼ˆä¾‹å¦‚ 3.5 â†’ 3.5000ï¼‰
    """
    item_cols_all = _find_item_cols(df_norm)
    if not item_cols_all:
        return pd.DataFrame()

    dims = sorted({d for d in (_dim_letter(c) for c in item_cols_all) if d is not None})

    df_item = df_norm[item_cols_all].apply(pd.to_numeric, errors="coerce")

    out = pd.DataFrame(index=df_norm.index)
    for d in dims:
        cols_d = [c for c in item_cols_all if _dim_letter(c) == d]
        mean_series = df_item[cols_d].mean(axis=1, skipna=True)
        out[d] = mean_series.apply(lambda x: f"{x:.4f}" if pd.notna(x) else "")

    return out


# ===== Regression table =====
def _sig_stars(p: float) -> str:
    if pd.isna(p):
        return ""
    if p < 0.001:
        return "***"
    if p < 0.01:
        return "**"
    if p < 0.05:
        return "*"
    return ""


def build_regression_table(df: pd.DataFrame, iv_vars: list[str], dv_var: str):
    """
    ç”¢ç”Ÿè¿´æ­¸è¡¨ï¼ˆæ¯”ç…§è«–æ–‡è¡¨æ ¼ï¼‰ï¼š
    - æœªæ¨™æº–åŒ–ä¿‚æ•¸ï¼ˆbï¼›æ¬„åä»ç”¨ã€ŒÎ²ä¼°è¨ˆå€¼ã€ä»¥ç¬¦åˆä½ çš„è¡¨é ­ï¼‰
    - æ¨™æº–åŒ–ä¿‚æ•¸ Betaï¼ˆBeta = b * sd(x) / sd(y)ï¼‰
    - tã€é¡¯è‘—æ€§(p)
    - Fã€P(F)ã€RÂ²ã€Adj RÂ²ã€N
    """
    if not iv_vars or not dv_var:
        raise ValueError("è«‹å…ˆè¨­å®šè‡ªè®Šæ•¸èˆ‡ä¾è®Šæ•¸ã€‚")

    cols = iv_vars + [dv_var]
    d = df[cols].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    y = d[dv_var].astype(float)
    X = d[iv_vars].astype(float)
    Xc = sm.add_constant(X, has_constant="add")

    model = sm.OLS(y, Xc).fit()

    params = model.params
    tvals = model.tvalues
    pvals = model.pvalues

    sd_y = y.std(ddof=1)
    sd_x = X.std(ddof=1)

    beta_std = {}
    for v in iv_vars:
        if sd_y == 0 or pd.isna(sd_y) or sd_x[v] == 0 or pd.isna(sd_x[v]):
            beta_std[v] = np.nan
        else:
            beta_std[v] = params[v] * (sd_x[v] / sd_y)

    rows = []
    rows.append(
        {
            "è‡ªè®Šé …": "ï¼ˆå¸¸æ•¸ï¼‰",
            "æœªæ¨™æº–åŒ–ä¿‚æ•¸ Î²ä¼°è¨ˆå€¼": f"{params['const']:.3f}",
            "æ¨™æº–åŒ–ä¿‚æ•¸ Beta": "â€”",
            "t": f"{tvals['const']:.3f}{_sig_stars(pvals['const'])}",
            "é¡¯è‘—æ€§": f"{pvals['const']:.3f}",
        }
    )

    for v in iv_vars:
        rows.append(
            {
                "è‡ªè®Šé …": v,
                "æœªæ¨™æº–åŒ–ä¿‚æ•¸ Î²ä¼°è¨ˆå€¼": f"{params[v]:.3f}",
                "æ¨™æº–åŒ–ä¿‚æ•¸ Beta": ("" if pd.isna(beta_std[v]) else f"{beta_std[v]:.3f}"),
                "t": f"{tvals[v]:.3f}{_sig_stars(pvals[v])}",
                "é¡¯è‘—æ€§": f"{pvals[v]:.3f}",
            }
        )

    table_df = pd.DataFrame(rows)

    summary = {
        "F": float(model.fvalue) if model.fvalue is not None else np.nan,
        "P(F)": float(model.f_pvalue) if model.f_pvalue is not None else np.nan,
        "R2": float(model.rsquared),
        "Adj_R2": float(model.rsquared_adj),
        "N": int(model.nobs),
    }
    return table_df, summary


# ===== Mediation analysis (IV -> M -> DV) =====
def _to_num_df(df: pd.DataFrame, cols: list[str]) -> pd.DataFrame:
    return df[cols].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")


def _fit_ols(y: pd.Series, X: pd.DataFrame):
    Xc = sm.add_constant(X, has_constant="add")
    return sm.OLS(y, Xc).fit()


def build_mediation_results(
    df: pd.DataFrame,
    iv: str,
    med: str,
    dv: str,
    n_boot: int = 2000,
    seed: int = 42,
):
    """
    ç”¢å‡ºä¸­ä»‹åˆ†æï¼ˆOLSï¼‰ï¼š
    - è·¯å¾‘ a: M ~ IV
    - è·¯å¾‘ c: DV ~ IV
    - è·¯å¾‘ b & c': DV ~ IV + M
    - indirect = a*b
    - Sobel z / pï¼ˆè¿‘ä¼¼ï¼‰
    - bootstrap CIï¼ˆpercentileï¼‰
    """
    d = _to_num_df(df, [iv, med, dv])
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/M/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    # a path
    m_a = _fit_ols(d[med], d[[iv]])
    a = float(m_a.params[iv])
    se_a = float(m_a.bse[iv])
    p_a = float(m_a.pvalues[iv])

    # c path (total)
    m_c = _fit_ols(d[dv], d[[iv]])
    c = float(m_c.params[iv])
    se_c = float(m_c.bse[iv])
    p_c = float(m_c.pvalues[iv])

    # b and c' path
    m_bc = _fit_ols(d[dv], d[[iv, med]])
    b = float(m_bc.params[med])
    se_b = float(m_bc.bse[med])
    p_b = float(m_bc.pvalues[med])

    c_prime = float(m_bc.params[iv])
    se_cprime = float(m_bc.bse[iv])
    p_cprime = float(m_bc.pvalues[iv])

    indirect = a * b

    
    # Sobel test (normal approximation)
    sobel_se = math.sqrt((b * b * se_a * se_a) + (a * a * se_b * se_b))
    sobel_z = (indirect / sobel_se) if sobel_se != 0 else float("nan")

    if np.isfinite(sobel_z):
        sobel_p = float(2 * (1 - norm.cdf(abs(sobel_z))))
    else:
        sobel_p = float("nan")

    # Bootstrap CI for indirect
    rng = np.random.default_rng(seed)
    n = len(d)
    inds = []
    for _ in range(int(n_boot)):
        idx = rng.integers(0, n, size=n)
        ds = d.iloc[idx]
        try:
            ma = _fit_ols(ds[med], ds[[iv]])
            mbc = _fit_ols(ds[dv], ds[[iv, med]])
            inds.append(float(ma.params[iv]) * float(mbc.params[med]))
        except Exception:
            continue

    if len(inds) >= 20:
        ci_low, ci_high = np.percentile(inds, [2.5, 97.5])
    else:
        ci_low, ci_high = (np.nan, np.nan)

    paths_df = pd.DataFrame(
        [
            {"è·¯å¾‘": "a (IVâ†’M)", "ä¿‚æ•¸": a, "SE": se_a, "t": float(m_a.tvalues[iv]), "p": p_a},
            {"è·¯å¾‘": "c (IVâ†’DV total)", "ä¿‚æ•¸": c, "SE": se_c, "t": float(m_c.tvalues[iv]), "p": p_c},
            {"è·¯å¾‘": "b (Mâ†’DV | IV)", "ä¿‚æ•¸": b, "SE": se_b, "t": float(m_bc.tvalues[med]), "p": p_b},
            {"è·¯å¾‘": "c' (IVâ†’DV direct | M)", "ä¿‚æ•¸": c_prime, "SE": se_cprime, "t": float(m_bc.tvalues[iv]), "p": p_cprime},
        ]
    )

    effects_df = pd.DataFrame(
        [
            {
                "æ•ˆæœ": "Indirect (a*b)",
                "å€¼": indirect,
                "Sobel z": sobel_z,
                "Sobel p": sobel_p,
                "Boot CI 2.5%": ci_low,
                "Boot CI 97.5%": ci_high,
            }
        ]
    )

    summary = {
        "N": int(n),
        "indirect": float(indirect),
        "sobel_z": float(sobel_z) if np.isfinite(sobel_z) else np.nan,
        "sobel_p": float(sobel_p) if np.isfinite(sobel_p) else np.nan,
        "ci_low": float(ci_low) if np.isfinite(ci_low) else np.nan,
        "ci_high": float(ci_high) if np.isfinite(ci_high) else np.nan,
        "boot_used": int(len(inds)),
    }
    return paths_df, effects_df, summary

from statsmodels.stats.stattools import durbin_watson

def _std_beta(params: pd.Series, X: pd.DataFrame, y: pd.Series) -> dict:
    """
    è¨ˆç®—æ¨™æº–åŒ–ä¿‚æ•¸ Betaï¼šBeta = b * sd(x) / sd(y)
    """
    sd_y = y.std(ddof=1)
    sd_x = X.std(ddof=1)
    out = {}
    for v in X.columns:
        if sd_y == 0 or pd.isna(sd_y) or sd_x[v] == 0 or pd.isna(sd_x[v]):
            out[v] = np.nan
        else:
            out[v] = float(params[v]) * float(sd_x[v] / sd_y)
    return out


def _fmt_beta(beta: float, p: float) -> str:
    if pd.isna(beta):
        return ""
    stars = _sig_stars(p)
    return f"{beta:.3f}{stars}"


def _fmt_t(t: float) -> str:
    if pd.isna(t):
        return ""
    return f"{t:.3f}"


def build_mediation_paper_table(df: pd.DataFrame, iv: str, med: str, dv: str):
    """
    ç”¢å‡ºè«–æ–‡å¼ä¸­ä»‹åˆ†æè¿´æ­¸è¡¨ï¼ˆå°æ‡‰ä½ å³é‚Šé‚£å¼µè¡¨ï¼‰ï¼š

    æ¢ä»¶äºŒï¼šDV=med, IV=[iv]
    æ¢ä»¶ä¸€ï¼šDV=dv,  IV=[iv]
    æ¢ä»¶ä¸‰ï¼šDV=dv,  IV=[iv, med]

    è¼¸å‡ºæ¬„ä½ï¼š
    - æ¯å€‹æ¢ä»¶ï¼šÎ²å€¼ï¼ˆæ¨™æº–åŒ–ä¿‚æ•¸ï¼‰èˆ‡ t å€¼
    - RÂ²ã€Î”RÂ²(=Adj RÂ²)ã€Fã€D-W
    """

    d = df[[iv, med, dv]].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/M/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    # ---- Condition 2: M ~ IV ----
    y2 = d[med].astype(float)
    X2 = d[[iv]].astype(float)
    m2 = _fit_ols(y2, X2)
    beta2 = _std_beta(m2.params, X2, y2)

    # ---- Condition 1: DV ~ IV ----
    y1 = d[dv].astype(float)
    X1 = d[[iv]].astype(float)
    m1 = _fit_ols(y1, X1)
    beta1 = _std_beta(m1.params, X1, y1)

    # ---- Condition 3: DV ~ IV + M ----
    y3 = d[dv].astype(float)
    X3 = d[[iv, med]].astype(float)
    m3 = _fit_ols(y3, X3)
    beta3 = _std_beta(m3.params, X3, y3)

    # æ¬„ä½åï¼ˆå°æ‡‰ä½ çš„è¡¨é ­æ›¿æ›ï¼‰
    col_c2_beta = f"{med}ï¼ˆæ¢ä»¶äºŒï¼‰Î²å€¼"
    col_c2_t    = f"{med}ï¼ˆæ¢ä»¶äºŒï¼‰tå€¼"
    col_c1_beta = f"{dv}ï¼ˆæ¢ä»¶ä¸€ï¼‰Î²å€¼"
    col_c1_t    = f"{dv}ï¼ˆæ¢ä»¶ä¸€ï¼‰tå€¼"
    col_c3_beta = f"{dv}ï¼ˆæ¢ä»¶ä¸‰ï¼‰Î²å€¼"
    col_c3_t    = f"{dv}ï¼ˆæ¢ä»¶ä¸‰ï¼‰tå€¼"

    # è¡¨æ ¼åˆ—ï¼šIV, M, RÂ², Î”RÂ²(Adj RÂ²), F, D-W
    rows = []

    # è‡ªè®Šé …ï¼ˆIVï¼‰åˆ—
    rows.append({
        "è‡ªè®Šé …": iv,
        col_c2_beta: _fmt_beta(beta2.get(iv, np.nan), float(m2.pvalues.get(iv, np.nan))),
        col_c2_t:    _fmt_t(float(m2.tvalues.get(iv, np.nan))),
        col_c1_beta: _fmt_beta(beta1.get(iv, np.nan), float(m1.pvalues.get(iv, np.nan))),
        col_c1_t:    _fmt_t(float(m1.tvalues.get(iv, np.nan))),
        col_c3_beta: _fmt_beta(beta3.get(iv, np.nan), float(m3.pvalues.get(iv, np.nan))),
        col_c3_t:    _fmt_t(float(m3.tvalues.get(iv, np.nan))),
    })

    # ä¸­ä»‹è®Šé …ï¼ˆMï¼‰åˆ—ï¼ˆåªæœ‰æ¢ä»¶ä¸‰æœ‰ï¼‰
    rows.append({
        "è‡ªè®Šé …": med,
        col_c2_beta: "",
        col_c2_t:    "",
        col_c1_beta: "",
        col_c1_t:    "",
        col_c3_beta: _fmt_beta(beta3.get(med, np.nan), float(m3.pvalues.get(med, np.nan))),
        col_c3_t:    _fmt_t(float(m3.tvalues.get(med, np.nan))),
    })

    # RÂ²
    rows.append({
        "è‡ªè®Šé …": "RÂ²",
        col_c2_beta: f"{float(m2.rsquared):.3f}",
        col_c2_t:    "",
        col_c1_beta: f"{float(m1.rsquared):.3f}",
        col_c1_t:    "",
        col_c3_beta: f"{float(m3.rsquared):.3f}",
        col_c3_t:    "",
    })

    # Î”RÂ²ï¼ˆä½ å³é‚Šè¡¨å…¶å¯¦æ˜¯ Adj RÂ²ï¼Œæ•¸å­—å·®å¾ˆå°ï¼š0.576 vs 0.575 é‚£ç¨®ï¼‰
    rows.append({
        "è‡ªè®Šé …": "Î”RÂ²",
        col_c2_beta: f"{float(m2.rsquared_adj):.3f}",
        col_c2_t:    "",
        col_c1_beta: f"{float(m1.rsquared_adj):.3f}",
        col_c1_t:    "",
        col_c3_beta: f"{float(m3.rsquared_adj):.3f}",
        col_c3_t:    "",
    })

    # F
    rows.append({
        "è‡ªè®Šé …": "F",
        col_c2_beta: f"{float(m2.fvalue):.3f}{_sig_stars(float(m2.f_pvalue))}",
        col_c2_t:    "",
        col_c1_beta: f"{float(m1.fvalue):.3f}{_sig_stars(float(m1.f_pvalue))}",
        col_c1_t:    "",
        col_c3_beta: f"{float(m3.fvalue):.3f}{_sig_stars(float(m3.f_pvalue))}",
        col_c3_t:    "",
    })

    # D-W
    rows.append({
        "è‡ªè®Šé …": "D-W",
        col_c2_beta: f"{float(durbin_watson(m2.resid)):.3f}",
        col_c2_t:    "",
        col_c1_beta: f"{float(durbin_watson(m1.resid)):.3f}",
        col_c1_t:    "",
        col_c3_beta: f"{float(durbin_watson(m3.resid)):.3f}",
        col_c3_t:    "",
    })

    table_df = pd.DataFrame(rows)

    meta = {
        "N": int(m3.nobs),
        "cond1": m1,
        "cond2": m2,
        "cond3": m3,
    }
    return table_df, meta


def build_moderation_paper_table(df: pd.DataFrame, iv: str, mod: str, dv: str):
    """
    ç”¢å‡ºè«–æ–‡å¼å¹²æ“¾åˆ†æè¿´æ­¸è¡¨ï¼ˆå°æ‡‰ä½ å³é‚Šé‚£å¼µè¡¨ï¼šæ¨¡å‹ä¸€/äºŒ/ä¸‰ï¼‰

    æ¨¡å‹ä¸€ï¼šDV ~ IV
    æ¨¡å‹äºŒï¼šDV ~ IV + MOD
    æ¨¡å‹ä¸‰ï¼šDV ~ IV + MOD + (IVÃ—MOD)

    è¼¸å‡ºæ¬„ä½ï¼š
    - æ¯å€‹æ¨¡å‹ï¼šÎ²å€¼ï¼ˆæ¨™æº–åŒ–ä¿‚æ•¸ï¼‰èˆ‡ t å€¼
    - RÂ²ã€Î”RÂ²ï¼ˆé€™è£¡æ˜¯ã€ŒRÂ² changeã€ï¼Œå°æ‡‰ä½ åœ–çš„ 0.063/0.001 é‚£ç¨®ï¼‰
    - F
    """

    d = df[[iv, mod, dv]].apply(pd.to_numeric, errors="coerce").dropna(axis=0, how="any")
    if d.empty:
        raise ValueError("å¯ç”¨è³‡æ–™ç‚ºç©ºï¼ˆIV/MOD/DV å¯èƒ½æœ‰ç©ºå€¼æˆ–éæ•¸å€¼ï¼‰ã€‚")

    # interaction termï¼ˆä¸åšä¸­å¿ƒåŒ–ï¼Œå®Œå…¨ç…§ä½ åœ–çš„åšæ³•ï¼›è‹¥ä½ è¦ä¸­å¿ƒåŒ–æˆ‘å¯å†åŠ  toggleï¼‰
    inter_name = f"{iv}Ã—{mod}"
    d[inter_name] = d[iv] * d[mod]

    # ---- Model 1: DV ~ IV ----
    y1 = d[dv].astype(float)
    X1 = d[[iv]].astype(float)
    m1 = _fit_ols(y1, X1)
    beta1 = _std_beta(m1.params, X1, y1)

    # ---- Model 2: DV ~ IV + MOD ----
    y2 = d[dv].astype(float)
    X2 = d[[iv, mod]].astype(float)
    m2 = _fit_ols(y2, X2)
    beta2 = _std_beta(m2.params, X2, y2)

    # ---- Model 3: DV ~ IV + MOD + IVÃ—MOD ----
    y3 = d[dv].astype(float)
    X3 = d[[iv, mod, inter_name]].astype(float)
    m3 = _fit_ols(y3, X3)
    beta3 = _std_beta(m3.params, X3, y3)

    # è¡¨é ­ï¼ˆå°é½Šä½ åœ–ï¼‰
    col_m1_beta = f"{dv}ï¼ˆæ¨¡å‹ä¸€ï¼‰Î²å€¼"
    col_m1_t    = f"{dv}ï¼ˆæ¨¡å‹ä¸€ï¼‰tå€¼"
    col_m2_beta = f"{dv}ï¼ˆæ¨¡å‹äºŒï¼‰Î²å€¼"
    col_m2_t    = f"{dv}ï¼ˆæ¨¡å‹äºŒï¼‰tå€¼"
    col_m3_beta = f"{dv}ï¼ˆæ¨¡å‹ä¸‰ï¼‰Î²å€¼"
    col_m3_t    = f"{dv}ï¼ˆæ¨¡å‹ä¸‰ï¼‰tå€¼"

    # Î”RÂ² = RÂ² changeï¼ˆæ¨¡å‹äºŒ-æ¨¡å‹ä¸€ï¼›æ¨¡å‹ä¸‰-æ¨¡å‹äºŒï¼›æ¨¡å‹ä¸€ç•™ç©ºæˆ–=RÂ²éƒ½è¡Œï¼‰
    
    r2_1 = float(m1.rsquared)
    r2_2 = float(m2.rsquared)
    r2_3 = float(m3.rsquared)

    # Î”RÂ² = RÂ² changeï¼ˆåš´æ ¼å®šç¾©ï¼‰
    dr2_1 = np.nan                # æ¨¡å‹ä¸€ä¸è¨ˆ Î”RÂ²ï¼ˆè«–æ–‡é€šå¸¸ç•™ç©ºï¼‰
    dr2_2 = r2_2 - r2_1           # æ¨¡å‹äºŒ âˆ’ æ¨¡å‹ä¸€
    dr2_3 = r2_3 - r2_2           # æ¨¡å‹ä¸‰ âˆ’ æ¨¡å‹äºŒ

    rows = []

    # IV row
    rows.append({
        "è‡ªè®Šé …": iv,
        col_m1_beta: _fmt_beta(beta1.get(iv, np.nan), float(m1.pvalues.get(iv, np.nan))),
        col_m1_t:    _fmt_t(float(m1.tvalues.get(iv, np.nan))),
        col_m2_beta: _fmt_beta(beta2.get(iv, np.nan), float(m2.pvalues.get(iv, np.nan))),
        col_m2_t:    _fmt_t(float(m2.tvalues.get(iv, np.nan))),
        col_m3_beta: _fmt_beta(beta3.get(iv, np.nan), float(m3.pvalues.get(iv, np.nan))),
        col_m3_t:    _fmt_t(float(m3.tvalues.get(iv, np.nan))),
    })

    # MOD row
    rows.append({
        "è‡ªè®Šé …": mod,
        col_m1_beta: "",
        col_m1_t:    "",
        col_m2_beta: _fmt_beta(beta2.get(mod, np.nan), float(m2.pvalues.get(mod, np.nan))),
        col_m2_t:    _fmt_t(float(m2.tvalues.get(mod, np.nan))),
        col_m3_beta: _fmt_beta(beta3.get(mod, np.nan), float(m3.pvalues.get(mod, np.nan))),
        col_m3_t:    _fmt_t(float(m3.tvalues.get(mod, np.nan))),
    })

    # Interaction row (IVÃ—MOD)
    rows.append({
        "è‡ªè®Šé …": f"{iv}*{mod}",
        col_m1_beta: "",
        col_m1_t:    "",
        col_m2_beta: "",
        col_m2_t:    "",
        col_m3_beta: _fmt_beta(beta3.get(inter_name, np.nan), float(m3.pvalues.get(inter_name, np.nan))),
        col_m3_t:    _fmt_t(float(m3.tvalues.get(inter_name, np.nan))),
    })

    # RÂ² row
    rows.append({
        "è‡ªè®Šé …": "RÂ²",
        col_m1_beta: f"{r2_1:.3f}",
        col_m1_t:    "",
        col_m2_beta: f"{r2_2:.3f}",
        col_m2_t:    "",
        col_m3_beta: f"{r2_3:.3f}",
        col_m3_t:    "",
    })

    # Î”RÂ² row (RÂ² change)
    rows.append({
    "è‡ªè®Šé …": "Î”RÂ²",
        col_m1_beta: "",
        col_m1_t:    "",
        col_m2_beta: f"{dr2_2:.3f}",
        col_m2_t:    "",
        col_m3_beta: f"{dr2_3:.3f}",
        col_m3_t:    "",
    })


    # F row
    rows.append({
        "è‡ªè®Šé …": "F",
        col_m1_beta: f"{float(m1.fvalue):.3f}{_sig_stars(float(m1.f_pvalue))}",
        col_m1_t:    "",
        col_m2_beta: f"{float(m2.fvalue):.3f}{_sig_stars(float(m2.f_pvalue))}",
        col_m2_t:    "",
        col_m3_beta: f"{float(m3.fvalue):.3f}{_sig_stars(float(m3.f_pvalue))}",
        col_m3_t:    "",
    })

    table_df = pd.DataFrame(rows)

    meta = {
        "N": int(m3.nobs),
        "interaction_col": inter_name,
    }
    return table_df, meta

from scipy.stats import pearsonr

def build_discriminant_validity_table(df_norm: pd.DataFrame, item_df: pd.DataFrame):
    """
    å€åˆ¥æ•ˆåº¦åˆ†æè¡¨ï¼ˆCorrelation Matrix + Cronbach's Î± on diagonalï¼‰

    - åˆ—ï¼æ¬„ï¼šå­æ§‹é¢ï¼ˆA1, A2, A3, B1, â€¦ï¼‰
    - å°è§’ç·šï¼šè©²å­æ§‹é¢æ•´é«” Cronbach's Î±
    - éå°è§’ç·šï¼ˆå·¦ä¸‹ï¼‰ï¼šå­æ§‹é¢å¹³å‡åˆ†æ•¸ä¹‹ Pearson correlation
    - å³ä¸Šä¸‰è§’ï¼šç•™ç©º
    """

    # 1ï¸âƒ£  å¾ item analysis çµæœæŠ“å­æ§‹é¢èˆ‡ alpha
    sub_alpha = (
        item_df
        .groupby("å­æ§‹é¢")["è©²å­æ§‹é¢æ•´é«” Î±"]
        .first()
        .dropna()
        .to_dict()
    )

    sub_dims = sorted(sub_alpha.keys())  # A1, A2, A3, ...

    # 2ï¸âƒ£ å»ºç«‹æ¯å€‹å­æ§‹é¢çš„ã€Œå¹³å‡åˆ†æ•¸ã€
    sub_scores = {}
    for sd in sub_dims:
        cols = [
            c for c in df_norm.columns
            if isinstance(c, str) and c.startswith(sd)
        ]
        if cols:
            sub_scores[sd] = (
                df_norm[cols]
                .apply(pd.to_numeric, errors="coerce")
                .mean(axis=1)
            )

    score_df = pd.DataFrame(sub_scores).dropna(axis=0, how="any")

    # 3ï¸âƒ£ å»ºç«‹ç©ºç™½è¡¨æ ¼
    mat = pd.DataFrame("", index=sub_dims, columns=sub_dims)

    # 4ï¸âƒ£ å¡«å€¼
    for i, r in enumerate(sub_dims):
        for j, c in enumerate(sub_dims):
            if i == j:
                # å°è§’ç·šï¼šCronbach's Î±
                try:
                    mat.loc[r, c] = f"{float(sub_alpha[r]):.3f}"
                except Exception:
                    mat.loc[r, c] = str(sub_alpha[r])
            elif i > j:
                # å·¦ä¸‹ä¸‰è§’ï¼šPearson r
                r_val, p_val = pearsonr(score_df[r], score_df[c])
                star = "**" if p_val < 0.01 else ""
                mat.loc[r, c] = f"{r_val:.3f}{star}"
            else:
                # å³ä¸Šä¸‰è§’ï¼šç•™ç©º
                mat.loc[r, c] = ""

    return mat


# ---- Sidebar ----
with st.sidebar:
    st.header("è¨­å®š")
    st.caption("1) ä¸Šå‚³ CSV â†’ 2) ç”¢å‡º Item Analysis â†’ 3) ä¸‹è¼‰çµæœï¼ˆCSVï¼‰")

    uploaded_file = st.file_uploader("ä¸Šå‚³ CSV", type=["csv"])

    st.divider()
    st.subheader("GPT è«–æ–‡å ±å‘Šç”Ÿæˆï¼ˆå¯é¸ï¼‰")

    gpt_on = st.toggle("å•Ÿç”¨ GPT å ±å‘Š", value=False, help="éœ€è¦ OpenAI API Key èˆ‡å¯ç”¨é¡åº¦ï¼ˆquotaï¼‰ã€‚")

    model_options = ["gpt-4o-mini", "gpt-4.1-mini", "gpt-4o", "gpt-4.1"]
    model_pick = st.selectbox("é¸æ“‡ GPT æ¨¡å‹", options=model_options, index=0)
    model_custom = st.text_input("æˆ–è‡ªè¡Œè¼¸å…¥æ¨¡å‹åç¨±ï¼ˆé¸å¡«ï¼‰", value="", placeholder="ä¾‹å¦‚ï¼šgpt-4o-mini")
    model_name = (model_custom.strip() or model_pick).strip()

    api_key = st.text_input("OpenAI API Keyï¼ˆä»¥ sk- é–‹é ­ï¼‰", type="password", value="")
    st.caption("å»ºè­°ç”¨ç’°å¢ƒè®Šæ•¸ä¹Ÿå¯ï¼šå…ˆåœ¨ç³»çµ±è¨­å®š OPENAI_API_KEYï¼Œå†ç•™ç©ºæ­¤æ¬„ã€‚")

    st.divider()
    st.subheader("å­æ§‹é¢è¦å‰‡ï¼ˆä½ æŒ‡å®šï¼‰")
    st.write("å­æ§‹é¢åªå–é¡Œé …ä»£ç¢¼çš„**å‰å…©ç¢¼**ï¼šä¾‹å¦‚ A01â†’A0ã€A11â†’A1ã€A105â†’A1")
    st.caption("â€» é€™å€‹è¦å‰‡éœ€ç”± analysis.py çš„åˆ†ç¾¤é‚è¼¯é…åˆï¼ˆè‹¥ä½ å·²æ”¹å¥½ analysis.py å°±æœƒç”Ÿæ•ˆï¼‰ã€‚")


# ---- Main ----
if uploaded_file is None:
    st.info("è«‹å…ˆåœ¨å·¦å´ä¸Šå‚³ CSV æª”æ¡ˆã€‚")
    st.stop()

try:
    df_raw = read_csv_safely(uploaded_file)
except Exception as e:
    safe_show_exception(e)
    st.stop()

# æ­£è¦åŒ–æ¬„åï¼ˆæ”¯æ´ A01.é¡Œç›® / A01 é¡Œç›® / A01ï¼‰
df_norm, mapping = normalize_item_columns(df_raw)

st.subheader("åŸå§‹è³‡æ–™é è¦½ï¼ˆå‰ 5 åˆ—ï¼‰")
st.dataframe(df_raw.head(), width="stretch")

with st.expander("æ¬„åæ­£è¦åŒ–å°ç…§ï¼ˆåŸå§‹æ¬„å â†’ é¡Œé …ä»£ç¢¼ï¼‰"):
    if mapping:
        map_df = pd.DataFrame([{"åŸå§‹æ¬„å": k, "é¡Œé …ä»£ç¢¼": v} for k, v in mapping.items()])
        st.dataframe(map_df, width="stretch")
    else:
        st.write("æœªåµæ¸¬åˆ°å¯æ­£è¦åŒ–çš„é¡Œé …æ¬„åï¼ˆè«‹ç¢ºèªæ¬„åæ ¼å¼ï¼‰ã€‚")

# ---- Item Analysis ----
st.subheader("ğŸ“ˆ Item Analysis çµæœ")

try:
    # =========================================================
    # 1ï¸âƒ£ Item Analysis
    # =========================================================
    result_df = run_item_analysis(df_norm)
    st.success("Item analysis completed.")
    st.dataframe(result_df, width="stretch", height=520)

    st.download_button(
        "ä¸‹è¼‰ Item Analysis çµæœ CSV",
        data=df_to_csv_bytes(result_df),
        file_name="item_analysis_results.csv",
        mime="text/csv",
    )

    # =========================================================
    # 2ï¸âƒ£ æ§‹é¢é€åˆ—å¹³å‡ï¼ˆåƒ…ä¾›åˆ†æä½¿ç”¨ï¼‰
    # =========================================================
    df_dim_means_row = build_dim_means_per_row(df_norm)
    if df_dim_means_row.empty:
        st.warning("æ‰¾ä¸åˆ°é¡Œé …ä»£ç¢¼æ¬„ä½ï¼Œç„¡æ³•ç”¢ç”Ÿæ§‹é¢å¹³å‡ï¼ˆA/B/C...ï¼‰ã€‚")
        st.stop()

    df_raw_plus_dimmeans = df_norm.copy()
    for c in df_dim_means_row.columns:
        df_raw_plus_dimmeans[c] = df_dim_means_row[c]

    dim_cols = list(df_dim_means_row.columns)

    # =========================================================
    # 3ï¸âƒ£ Discriminant Validityï¼ˆç¨ç«‹ try / exceptï¼‰
    # =========================================================
    st.divider()
    st.subheader("ğŸ“Š å€åˆ¥æ•ˆåº¦åˆ†æè¡¨")

    try:
        disc_df = build_discriminant_validity_table(df_norm, result_df)

        st.dataframe(disc_df, width="stretch")
        st.caption(
            "è¨»ï¼šå°è§’ç·šç‚ºå„å­æ§‹é¢ä¹‹ Cronbachâ€™s Î±ï¼›"
            "å·¦ä¸‹ä¸‰è§’ç‚ºå­æ§‹é¢é–“ä¹‹çš®çˆ¾æ£®ç›¸é—œä¿‚æ•¸ï¼ˆ** P<0.01ï¼‰ã€‚"
        )

        st.download_button(
            "ä¸‹è¼‰ å€åˆ¥æ•ˆåº¦åˆ†æè¡¨ CSV",
            data=df_to_csv_bytes(disc_df),
            file_name="discriminant_validity_table.csv",
            mime="text/csv",
        )

    except Exception as e:
        st.error("å€åˆ¥æ•ˆåº¦åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
        safe_show_exception(e)

    # =========================================================
    # 4ï¸âƒ£ ç ”ç©¶è®Šæ•¸è¨­å®šï¼ˆIV / DVï¼‰
    # =========================================================
    st.divider()
    st.subheader("ğŸ“Œ ç ”ç©¶è®Šæ•¸è¨­å®šï¼ˆè‡ªè®Šæ•¸ / ä¾è®Šæ•¸ï¼‰")

    iv_vars = st.multiselect(
        "â‘  å‹¾é¸è‡ªè®Šæ•¸ï¼ˆå¯è¤‡é¸ï¼‰",
        options=dim_cols,
        default=[],
    )

    dv_var = st.selectbox(
        "â‘¡ é¸æ“‡ä¾è®Šæ•¸ï¼ˆå–®ä¸€ï¼‰",
        options=[""] + dim_cols,
        index=0,
    )

    if dv_var and dv_var in iv_vars:
        st.error("âš ï¸ ä¾è®Šæ•¸ä¸å¯åŒæ™‚è¢«é¸ç‚ºè‡ªè®Šæ•¸ï¼Œè«‹é‡æ–°è¨­å®šã€‚")

    elif iv_vars and dv_var:
        st.success(f"ç ”ç©¶æ¨¡å‹ï¼šIV = {iv_vars} â†’ DV = {dv_var}")

        df_research = df_raw_plus_dimmeans[iv_vars + [dv_var]].copy()
        st.dataframe(df_research, width="stretch")

        st.download_button(
            "ä¸‹è¼‰ ç ”ç©¶ç”¨è³‡æ–™ CSVï¼ˆIV + DVï¼‰",
            data=df_to_csv_bytes(df_research),
            file_name="research_dataset_IV_DV.csv",
            mime="text/csv",
        )

        # =====================================================
        # 5ï¸âƒ£ Regression
        # =====================================================
        st.divider()
        st.subheader("ğŸ“Š è¿´æ­¸åˆ†æè¡¨ï¼ˆè«–æ–‡æ ¼å¼ï¼‰")

        if st.button("åŸ·è¡Œè¿´æ­¸åˆ†æ", type="primary"):
            try:
                reg_table, reg_sum = build_regression_table(
                    df_research, iv_vars, dv_var
                )

                st.dataframe(reg_table, width="stretch")
                st.markdown(
                    f"**F={reg_sum['F']:.3f}ï¼ŒP={reg_sum['P(F)']:.3f}ï¼Œ"
                    f"RÂ²={reg_sum['R2']:.3f}ï¼ŒAdj RÂ²={reg_sum['Adj_R2']:.3f}ï¼Œ"
                    f"N={reg_sum['N']}**"
                )

            except Exception as e:
                st.error("è¿´æ­¸åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
                safe_show_exception(e)

    else:
        st.info("è«‹å…ˆé¸æ“‡è‡³å°‘ä¸€å€‹è‡ªè®Šæ•¸èˆ‡ä¸€å€‹ä¾è®Šæ•¸ã€‚")

except Exception as e:
    st.error("Item Analysis ä¸»æµç¨‹å¤±æ•—ï¼ˆsafeï¼‰")
    safe_show_exception(e)
    st.stop()

# ====== Mediation Settings (äº’æ–¥ï¼šA/B/C/D... åªèƒ½å‡ºç¾åœ¨ä¸€å€‹ä½ç½®) ======
st.divider()
st.subheader("ğŸ§© ä¸­ä»‹åˆ†æè¨­å®š")

dim_cols_all = dim_cols  # A, B, C, D ...

col1, col2, col3 = st.columns(3)

with col1:
    iv_m = st.selectbox(
        "â‘  è‡ªè®Šæ•¸ï¼ˆIVï¼‰",
        options=[""] + dim_cols_all,
        index=0,
        key="med_iv",
    )

with col2:
    med_options = [""] + [c for c in dim_cols_all if c != iv_m]
    med_m = st.selectbox(
        "â‘¡ ä¸­ä»‹è®Šæ•¸ï¼ˆMï¼‰",
        options=med_options,
        index=0,
        key="med_m",
    )

with col3:
    dv_options = [""] + [c for c in dim_cols_all if c not in {iv_m, med_m}]
    dv_m = st.selectbox(
        "â‘¢ ä¾è®Šæ•¸ï¼ˆDVï¼‰",
        options=dv_options,
        index=0,
        key="med_dv",
    )

chosen = [x for x in [iv_m, med_m, dv_m] if x]

if len(chosen) != len(set(chosen)):
    st.error("âš ï¸ IV / M / DV ä¸å¯é‡è¤‡ï¼ŒAã€Bã€Cã€Dâ€¦ æ¯å€‹åªèƒ½å‡ºç¾åœ¨ä¸€å€‹è§’è‰²ä¸­ã€‚")

elif iv_m and med_m and dv_m:
    st.success(f"ä¸­ä»‹æ¨¡å‹ï¼š{iv_m} â†’ {med_m} â†’ {dv_m}")

    st.markdown("### ç ”ç©¶ç”¨è³‡æ–™è¡¨ï¼ˆåƒ…ä¿ç•™ IV / M / DVï¼‰")
    df_mediation = df_raw_plus_dimmeans[[iv_m, med_m, dv_m]].copy()
    st.dataframe(df_mediation, width="stretch")

    st.download_button(
        "ä¸‹è¼‰ ä¸­ä»‹åˆ†æç ”ç©¶ç”¨è³‡æ–™ CSVï¼ˆIV + M + DVï¼‰",
        data=df_to_csv_bytes(df_mediation),
        file_name=f"mediation_dataset_{iv_m}_{med_m}_{dv_m}.csv",
        mime="text/csv",
    )

    st.markdown("### ä¸­ä»‹åˆ†æ")

    n_boot = st.number_input(
        "Bootstrap æ¬¡æ•¸ï¼ˆå»ºè­° 2000ï¼‰",
        min_value=200,
        max_value=20000,
        value=2000,
        step=200,
    )

    if st.button("åŸ·è¡Œä¸­ä»‹åˆ†æ", type="primary", key="run_mediation"):
        try:
            paper_table, meta = build_mediation_paper_table(
                df_raw_plus_dimmeans,
                iv=iv_m,
                med=med_m,
                dv=dv_m,
            )

            st.markdown(
                f"### ä¸­ä»‹è®Šæ•¸ï¼ˆ{med_m}ï¼‰å° è‡ªè®Šæ•¸ï¼ˆ{iv_m}ï¼‰èˆ‡ ä¾è®Šæ•¸ï¼ˆ{dv_m}ï¼‰ä¹‹ä¸­ä»‹åˆ†æè¡¨"
            )

            st.dataframe(paper_table, width="stretch")

            st.caption(
                "è¨»ï¼š* P<0.05ï¼Œ** P<0.01ï¼Œ*** P<0.001ï¼›"
                "Î”RÂ² ç‚ºèª¿æ•´å¾Œ RÂ²ï¼ˆAdj RÂ²ï¼‰ï¼›D-W ç‚º Durbinâ€“Watsonã€‚"
            )

            tag = f"{iv_m}_to_{med_m}_to_{dv_m}".replace(" ", "")
            st.download_button(
                "ä¸‹è¼‰ ä¸­ä»‹åˆ†æè¡¨ CSV",
                data=df_to_csv_bytes(paper_table),
                file_name=f"mediation_table_{tag}.csv",
                mime="text/csv",
            )

            st.markdown(f"**N={meta['N']}**")

        except Exception as e:
            st.error("ä¸­ä»‹åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
            safe_show_exception(e)

else:
    st.info("è«‹ä¾åºé¸æ“‡ IV / M / DVï¼ˆä¸”ä¸‰è€…ä¸å¯é‡è¤‡ï¼‰å¾Œï¼Œæ‰æœƒé¡¯ç¤ºä¸­ä»‹åˆ†æè³‡æ–™èˆ‡çµæœã€‚")



# =========================
# Moderation (IV -> DV moderated by W)
# =========================
st.divider()
st.subheader("ğŸ§© å¹²æ“¾åˆ†æè¨­å®š")

col1, col2, col3 = st.columns(3)

with col1:
    iv_w = st.selectbox("â‘  è‡ªè®Šæ•¸ï¼ˆIVï¼‰", options=[""] + dim_cols, index=0, key="mod_iv")

# moderator options exclude IV
mod_options = [""] + [c for c in dim_cols if c != iv_w]
with col2:
    w_var = st.selectbox("â‘¡ å¹²æ“¾è®Šæ•¸ï¼ˆWï¼‰", options=mod_options, index=0, key="mod_w")

# dv options exclude IV & W
dv_options2 = [""] + [c for c in dim_cols if c not in {iv_w, w_var}]
with col3:
    dv_w = st.selectbox("â‘¢ ä¾è®Šæ•¸ï¼ˆDVï¼‰", options=dv_options2, index=0, key="mod_dv")

chosen2 = [x for x in [iv_w, w_var, dv_w] if x]
if len(chosen2) != len(set(chosen2)):
    st.error("âš ï¸ IV / W / DV ä¸å¯é‡è¤‡ï¼ŒAã€Bã€Cã€Dâ€¦ æ¯å€‹åªèƒ½å‡ºç¾åœ¨ä¸€å€‹è§’è‰²ä¸­ã€‚")
else:
    if iv_w and w_var and dv_w:
        st.success(f"å¹²æ“¾æ¨¡å‹ï¼š{iv_w} â†’ {dv_w}ï¼ˆW={w_var}ï¼‰")

        st.markdown("### ç ”ç©¶ç”¨è³‡æ–™è¡¨ï¼ˆåƒ…ä¿ç•™ IV / W / DVï¼‰")
        df_moderation = df_raw_plus_dimmeans[[iv_w, w_var, dv_w]].copy()
        st.dataframe(df_moderation, width="stretch")

        st.download_button(
            "ä¸‹è¼‰ å¹²æ“¾åˆ†æç ”ç©¶ç”¨è³‡æ–™ CSVï¼ˆIV + W + DVï¼‰",
            data=df_to_csv_bytes(df_moderation),
            file_name=f"moderation_dataset_{iv_w}_{w_var}_{dv_w}.csv",
            mime="text/csv",
        )

        run_mod = st.button("åŸ·è¡Œå¹²æ“¾åˆ†æ", type="primary", key="run_moderation")

        if run_mod:
            try:
                mod_table, mod_meta = build_moderation_paper_table(
                    df_raw_plus_dimmeans, iv=iv_w, mod=w_var, dv=dv_w
                )

                # âœ… ä½ æŒ‡å®šçš„æ¨™é¡Œ
                st.markdown(
                    f"### å¹²æ“¾è®Šæ•¸ï¼ˆ{w_var}ï¼‰å° è‡ªè®Šæ•¸ï¼ˆ{iv_w}ï¼‰èˆ‡ ä¾è®Šæ•¸ï¼ˆ{dv_w}ï¼‰ä¹‹å¹²æ“¾åˆ†æè¡¨"
                )

                st.dataframe(mod_table, width="stretch")
                st.caption("è¨»ï¼š* P<0.05ï¼Œ** P<0.01ï¼Œ*** P<0.001ï¼›Î”RÂ² ç‚º RÂ² è®ŠåŒ–é‡ï¼ˆRÂ² changeï¼‰ã€‚")

                tag2 = f"{iv_w}_x_{w_var}_to_{dv_w}".replace(" ", "")
                st.download_button(
                    "ä¸‹è¼‰ å¹²æ“¾åˆ†æè¡¨ CSV",
                    data=df_to_csv_bytes(mod_table),
                    file_name=f"moderation_table_{tag2}.csv",
                    mime="text/csv",
                )

                st.markdown(f"**N={mod_meta['N']}**")

            except Exception as e:
                st.error("å¹²æ“¾åˆ†æå¤±æ•—ï¼ˆsafeï¼‰")
                safe_show_exception(e)

    else:
        st.info("è«‹ä¾åºé¸æ“‡ IV / W / DVï¼ˆä¸”ä¸‰è€…ä¸å¯é‡è¤‡ï¼‰å¾Œï¼Œæ‰æœƒé¡¯ç¤ºå¹²æ“¾åˆ†æè³‡æ–™èˆ‡çµæœã€‚")



# ---- GPT report (optional) ----
st.divider()
st.subheader("ğŸ“ GPT è«–æ–‡å ±å‘Šç”Ÿæˆï¼ˆæ–‡å­—ï¼‰")

if not gpt_on:
    st.info("ä½ ç›®å‰æœªå•Ÿç”¨ GPT å ±å‘Šã€‚è‹¥è¦ç”Ÿæˆè«–æ–‡æ–‡å­—ï¼Œè«‹åœ¨å·¦å´æ‰“é–‹ã€Œå•Ÿç”¨ GPT å ±å‘Šã€ã€‚")
    st.stop()

if not GPT_AVAILABLE:
    st.warning("æ‰¾ä¸åˆ°å¯ç”¨çš„ generate_gpt_reportï¼ˆè«‹ç¢ºèª gpt_report.py ä¸­æœ‰å®šç¾© generate_gpt_reportï¼‰ã€‚")
    st.stop()

key = (api_key or os.getenv("OPENAI_API_KEY") or "").strip()
if not key:
    st.warning("å°šæœªæä¾› OpenAI API Keyã€‚è«‹åœ¨å·¦å´è¼¸å…¥ï¼Œæˆ–è¨­å®šç’°å¢ƒè®Šæ•¸ OPENAI_API_KEYã€‚")
    st.stop()

gen = st.button("ç”Ÿæˆ GPT å ±å‘Šï¼ˆæ–‡å­—ï¼‰", type="primary")

if gen:
    try:
        report = generate_gpt_report(result_df, model=model_name, api_key=key)

        paper_text = None
        if isinstance(report, dict):
            paper_text = report.get("paper_text") or report.get("text") or report.get("output")
        elif isinstance(report, str):
            paper_text = report

        if not paper_text:
            st.warning("GPT å›å‚³å…§å®¹ç‚ºç©ºï¼Œè«‹æª¢æŸ¥ gpt_report.py çš„å›å‚³æ ¼å¼ã€‚")
        else:
            st.success("GPT å ±å‘Šç”Ÿæˆå®Œæˆã€‚")
            st.text_area("GPT è«–æ–‡å ±å‘Šï¼ˆå¯è¤‡è£½ï¼‰", value=paper_text, height=420)

            st.download_button(
                "ä¸‹è¼‰ GPT å ±å‘Š TXT",
                data=paper_text.encode("utf-8"),
                file_name="gpt_paper_report.txt",
                mime="text/plain",
            )

    except Exception as e:
        msg = repr(e)
        if "insufficient_quota" in msg or "You exceeded your current quota" in msg:
            st.error("GPT report failedï¼šä½ çš„ OpenAI API å¸³è™Ÿç›®å‰æ²’æœ‰å¯ç”¨é¡åº¦ï¼ˆinsufficient_quotaï¼‰ã€‚")
            st.caption("è§£æ³•ï¼šåˆ° OpenAI å¹³å° Billing/Credits åŠ å€¼å¾Œå†è©¦ã€‚")
        else:
            st.error("GPT report failed. See error details below (safe).")
            safe_show_exception(e)
